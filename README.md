# Poll Automation App

Poll Automation App is a standalone, open-source web application designed to intelligently generate and manage live polls in real-time during lectures, webinars, or meetings â€” without being tied to any specific video conferencing platform.

## ðŸ“ Monorepo Folder Structure (Turborepo)

```
poll-automation/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ frontend/         # Vite React TypeScript frontend
â”‚   â””â”€â”€ backend/          # Express/Vite backend
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ whisper/          # Python service for audio transcription (Whisper)
â”‚   â””â”€â”€ pollgen-llm/      # Poll generation logic using API/Local LLMs
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ types/            # Shared TypeScript types
â”‚   â””â”€â”€ utils/            # Shared utility functions
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/        # CI/CD pipelines
â”œâ”€â”€ package.json          # Root config with workspaces
â”œâ”€â”€ turbo.json            # Turborepo pipeline config
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ðŸš€ Getting Started

### ðŸ”§ Python Environment Setup

1. **Navigate to the Whisper service folder:**

```bash
cd services/whisper
```

2. **Create and activate a Python virtual environment:**

```bash
# Windows
python -m venv whisper-env
whisper-env\Scripts\activate

# macOS/Linux
python3 -m venv whisper-env
source whisper-env/bin/activate
```

3.1 **For CPU-only**

```bash
pip install --upgrade pip
pip install -r requirements.txt
````

This installs everything except large GPU-related packages like `torch`.
Useful for quickly running the backend in **CPU mode** for testing or development.


3.2 **âš¡ For GPU support (CUDA 12.1)**

If you have a CUDA-enabled GPU and want to use GPU acceleration:

```bash
pip install -r requirements.gpu.txt --extra-index-url https://download.pytorch.org/whl/cu121
```

This will install `torch`, `torchaudio`, and `torchvision` with CUDA 12.1 support.
Make sure your system has the correct CUDA runtime installed.


## ðŸ”§ .env Configuration

### `apps/backend/.env`

```
PORT=3000
WHISPER_WS_URL=ws://localhost:8000
```

### `apps/frontend/.env`

```
VITE_BACKEND_WS_URL=ws://localhost:3000
```

### Global Prerequisites
**Navigate to the root directory:**

Install `pnpm` and `turbo` globally (once):

```bash
npm install -g pnpm
pnpm add -g turbo
```
### 1. Install dependencies

```bash
pnpm install
```

### 2. Start all dev servers

```bash
pnpm dev
```
This starts:

* âœ… *Frontend* â†’ [http://localhost:5173](http://localhost:5173)
* âœ… *Backend (WebSocket server)* â†’ ws\://localhost:3000
* âœ… *Whisper Transcription Service* â†’ ws\://localhost:8000 (Python FastAPI)

> Make sure the Python environment is set up correctly (faster-whisper, uvicorn, etc.)

## ðŸ›† Using Turborepo

* `pnpm build` â†’ Build all apps/services
* `pnpm lint` â†’ Lint all projects
* `pnpm test` â†’ Run tests
* `turbo run <task>` â†’ Run any task across monorepo


## ðŸ—£ Phase 1 â€“ Transcription Pipeline

> This outlines the current real-time transcription flow:

1. **Frontend** records or selects a `.wav` file and sends it over WebSocket (binary + metadata).
2. **Backend** WebSocket server receives and forwards it to the Whisper service.
3. **Whisper Service** processes audio using Faster-Whisper and returns transcription in JSON.
4. **Backend** sends transcription JSON back to the frontend or passes it to the LLM service.

> Currently, the transcription is **not displayed** to the user â€“ it is **used internally** to generate polls using an LLM.

ðŸ“… Upcoming Phases:

* Phase 2: LLM-based Poll Generation
* Phase 3: Realtime Poll Launch and Analytics

